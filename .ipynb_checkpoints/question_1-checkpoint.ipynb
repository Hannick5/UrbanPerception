{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2b7531",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49107272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import shutil\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Subtract, concatenate, Input, Flatten, Activation, Dense, Dropout, Lambda, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49152eb",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e28fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    \n",
    "def plot_loss(history):\n",
    "    # Historique des valeurs de précision d'entraînement et de validation\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Historique des numéros d'époque\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Tracer la courbe de précision d'entraînement\n",
    "    plt.plot(epochs, train_loss, 'b', label='Train Loss')\n",
    "    # Tracer la courbe de précision de validation\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_accuracy(history):\n",
    "    # Historique des valeurs de précision d'entraînement et de validation\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Historique des numéros d'époque\n",
    "    epochs = range(1, len(train_accuracy) + 1)\n",
    "\n",
    "    # Tracer la courbe de précision d'entraînement\n",
    "    plt.plot(epochs, train_accuracy, 'b', label='Train Accuracy')\n",
    "    # Tracer la courbe de précision de validation\n",
    "    plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "    \n",
    "def predicting_on_dataset(X_pred, model):\n",
    "    # Select a subset of the test data for visualization\n",
    "    subset_size = 300\n",
    "    X_subset = X_pred[:subset_size]\n",
    "\n",
    "    # Make predictions on the subset of test data\n",
    "    predictions = model.predict(X_subset)\n",
    "\n",
    "    # Plot the images and predictions\n",
    "    fig, axes = plt.subplots(subset_size, 2, figsize=(10, subset_size*2))\n",
    "    for i in range(subset_size):\n",
    "        # Plot first image\n",
    "        axes[i, 0].imshow(X_subset[0][i])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot second image\n",
    "        axes[i, 1].imshow(X_subset[1][i])\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Add predicted score as title\n",
    "        score = predictions[i]  # Assuming the second element represents the score\n",
    "        axes[i, 1].set_title(score)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def data_aug(train_left, train_right, train_label, nb, save_folder):\n",
    "\n",
    "    # Create saving folder\n",
    "    aug_folder = safe_folder_creation(os.path.join(save_folder))\n",
    "\n",
    "    # Specify data generator parameters\n",
    "    datagenargs = {\n",
    "        'rotation_range': 2, 'width_shift_range': 0.2, 'height_shift_range': 0.2,\n",
    "        'shear_range': 0.1,\n",
    "        'zoom_range': 0.25, 'horizontal_flip': True, 'fill_mode': 'nearest'\n",
    "    }\n",
    "\n",
    "    #  Create generators\n",
    "    left_datagen = ImageDataGenerator(**datagenargs)\n",
    "    right_datagen = ImageDataGenerator(**datagenargs)\n",
    "\n",
    "    # Initialization of data\n",
    "    train_left_aug = list(train_left)\n",
    "    train_right_aug = list(train_right)\n",
    "    train_label_aug = list(train_label)\n",
    "    img_size = train_left[0].shape[0]\n",
    "\n",
    "    # Display processing advancement\n",
    "    print(\"Creating new inputs...\")\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    # Create nb augmented images from an original one\n",
    "    for duel in pbar(range(len(train_label))):\n",
    "        for _ in range(nb):\n",
    "            # Create one augmented image from the left one\n",
    "            ori_left_img = train_left[duel]\n",
    "            left_img = ori_left_img.reshape((1,) + ori_left_img.shape)\n",
    "            aug_img = left_datagen.flow(left_img, batch_size=1)\n",
    "            left_aug_img = aug_img[0].reshape(ori_left_img.shape)\n",
    "\n",
    "            # Create one augmented image from the right one\n",
    "            ori_right_img = train_right[duel]\n",
    "            right_img = ori_right_img.reshape((1,) + ori_right_img.shape)\n",
    "            aug_img = right_datagen.flow(right_img, batch_size=1)\n",
    "            right_aug_img = aug_img[0].reshape(ori_right_img.shape)\n",
    "\n",
    "            # Add to list\n",
    "            train_left_aug.append(left_aug_img)\n",
    "            train_right_aug.append(right_aug_img)\n",
    "            train_label_aug.append(train_label[duel])\n",
    "\n",
    "    # Convert to array\n",
    "    train_left_aug = np.array(train_left_aug)\n",
    "    train_right_aug = np.array(train_right_aug)\n",
    "    train_label_aug = np.array(train_label_aug)\n",
    "    train_data_aug = [train_left_aug, train_right_aug]\n",
    "    \n",
    "    return train_data_aug, train_label_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2752818",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data\\question_1\\duels_question_1.csv\",usecols=[0,1,2], header=None)\n",
    "data.columns = [\"Image 1\", \"Image 2\", \"labels\"]\n",
    "\n",
    "#Deleting the no preference data\n",
    "data = data[data[\"labels\"] != \"No preference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d321d1",
   "metadata": {},
   "source": [
    "### Splitting and formatting the data for the comparison model using the duels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 224\n",
    "\n",
    "def prepare_dataset_arrays(image_folder, data, shape):\n",
    "\n",
    "    image1_names = data.iloc[:,0].values\n",
    "    image2_names = data.iloc[:,1].values\n",
    "    labels = data.iloc[:,2].values\n",
    "\n",
    "    image1_array = []\n",
    "    image2_array = []\n",
    "    \n",
    "    for image1_name, image2_name in zip(image1_names, image2_names):\n",
    "        for filename in os.listdir(image_folder):\n",
    "            if image1_name in filename:\n",
    "                image1_path = os.path.join(image_folder, filename)\n",
    "                image1 = cv2.imread(image1_path)\n",
    "                image1 = cv2.resize(image1, (shape, shape))\n",
    "                image1 = image1.astype(np.float32) / 255.0\n",
    "                image1_array.append(image1)\n",
    "            elif image2_name in filename:\n",
    "                image2_path = os.path.join(image_folder, filename)\n",
    "                image2 = cv2.imread(image2_path)\n",
    "                image2 = cv2.resize(image2, (shape, shape))\n",
    "                image2 = image2.astype(np.float32) / 255.0\n",
    "                image2_array.append(image2)\n",
    "                \n",
    "    return image1_array, image2_array, labels\n",
    "\n",
    "image1_array, image2_array, labels = prepare_dataset_arrays(\"data\\question_1\\Sample_web_green\", data, shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e839a",
   "metadata": {},
   "source": [
    "### Creating the prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_siamese(directory, shape):\n",
    "    image_pred = []\n",
    "    for img in glob.glob(directory):\n",
    "        image1 = cv2.imread(img)\n",
    "        image1 = cv2.resize(image1, (shape, shape))\n",
    "        image1 = image1.astype(np.float32) / 255.0\n",
    "        image_pred.append(image1)\n",
    "    \n",
    "    image_pred_1 = tf.convert_to_tensor(np.array(image_pred[:300])) \n",
    "    image_pred_2 = tf.convert_to_tensor(np.array(image_pred[300:600])) \n",
    "\n",
    "    X_pred = [image_pred_1, image_pred_2]\n",
    "    \n",
    "    return X_pred\n",
    "\n",
    "X_pred = prepare_prediction_siamese(\"data/question_1/ForPrediction/*/*\", shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb592f8",
   "metadata": {},
   "source": [
    "### Creating the Training, Validation and Testing datasets with a split of (60%, 20%, 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e808894",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "def prepare_dataset_for_network(image1_array, image2_array, labels):\n",
    "    # Format the labels of left and right images\n",
    "    labels_formatted = []\n",
    "\n",
    "    for label in labels:\n",
    "        if label == \"left\":\n",
    "            labels_formatted.append([1,0])  # 0 represents left image\n",
    "        elif label == \"right\":\n",
    "            labels_formatted.append([0,1])  # 1 represents right image\n",
    "\n",
    "    labels_formatted = np.array(labels_formatted)\n",
    "\n",
    "    # Conversion of the lists into numpy arrays\n",
    "    image1_array = np.array(image1_array)\n",
    "    image2_array = np.array(image2_array)\n",
    "\n",
    "    labels_formatted = tf.convert_to_tensor(labels_formatted)\n",
    "\n",
    "    image1_array = tf.convert_to_tensor(image1_array)\n",
    "    image2_array = tf.convert_to_tensor(image2_array)\n",
    "    \n",
    "    # Split the data into training, validation, and test sets using array slicing\n",
    "    train_size = int(0.6 * len(image1_array))\n",
    "    valid_size = int(0.2 * len(image1_array))\n",
    "\n",
    "    X_train = [image1_array[:train_size], image2_array[:train_size]]\n",
    "    y_train = labels_formatted[:train_size]\n",
    "\n",
    "    X_valid = [image1_array[train_size:train_size + valid_size], image2_array[train_size:train_size + valid_size]]\n",
    "    y_valid = labels_formatted[train_size:train_size + valid_size]\n",
    "\n",
    "    X_test = [image1_array[train_size + valid_size:], image2_array[train_size + valid_size:]]\n",
    "    y_test = labels_formatted[train_size + valid_size:]\n",
    "    \n",
    "    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid), (X_test, y_test) = prepare_dataset_for_network(image1_array, image2_array, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bce16b",
   "metadata": {},
   "source": [
    "# Building the siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17103d",
   "metadata": {},
   "source": [
    "## Building the model for the comparison between the two pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ecb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def comparison_siamese_model(input_shape):\n",
    "\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    # Create inputs for pairs of images\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    input_2 = Input(shape=input_shape)\n",
    "\n",
    "    # Get embeddings of the images using the shared VGG19 model\n",
    "    output_1 = base_model(input_1)\n",
    "    output_2 = base_model(input_2)\n",
    "\n",
    "    concat = concatenate([output_1, output_2])\n",
    "\n",
    "    # Classification layer to predict similarity\n",
    "    flatten = Flatten()(concat)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(concat)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the complete siamese model\n",
    "    siamese_model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "    # Compile the model\n",
    "    siamese_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.000001), metrics=['accuracy'])\n",
    "\n",
    "    # Print model summary\n",
    "    siamese_model.summary()\n",
    "    \n",
    "    return siamese_model\n",
    "\n",
    "# Train the siamese network\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "siamese_comparison_model = comparison_siamese_model((224, 224, 3))\n",
    "\n",
    "history = siamese_comparison_model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_valid, y_valid), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f46ae",
   "metadata": {},
   "source": [
    "### Testing accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc476623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_accuracy(siamese_comparison_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c5349",
   "metadata": {},
   "source": [
    "### Plotting the accuracy metric for the validation and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f88745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76909c",
   "metadata": {},
   "source": [
    "### Plotting the loss for the validation and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dab96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24966341",
   "metadata": {},
   "source": [
    "### Plotting some of the results from the prediction on the Prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting_on_dataset([X_pred[0],X_pred[0]], siamese_comparison_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04b268",
   "metadata": {},
   "source": [
    "# Building the ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label_for_ranking(labels):\n",
    "    # Format the labels of left and right images\n",
    "    labels_formatted = []\n",
    "\n",
    "    for label in labels:\n",
    "        if label == \"left\":\n",
    "            labels_formatted.append(0)  # 0 represents left image\n",
    "        elif label == \"right\":\n",
    "            labels_formatted.append(1)  # 1 represents right image\n",
    "\n",
    "    labels_formatted = np.array(labels_formatted)\n",
    "    labels_formatted = tf.convert_to_tensor(labels_formatted)\n",
    "    # Split the data into training, validation, and test sets using array slicing\n",
    "    train_size = int(0.6 * len(image1_array))\n",
    "    valid_size = int(0.2 * len(image1_array))\n",
    "    \n",
    "    y_train = labels_formatted[:train_size]\n",
    "    y_valid = labels_formatted[train_size:train_size + valid_size]\n",
    "    y_test = labels_formatted[train_size + valid_size:]\n",
    "    \n",
    "    return y_train, y_valid, y_test\n",
    "\n",
    "y_train, y_valid, y_test = prepare_label_for_ranking(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b8810",
   "metadata": {},
   "source": [
    "### Guillaume's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa78f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_ranking_network(img_size):\n",
    "    \"\"\"\n",
    "    Create ranking network which give a score to an image.\n",
    "\n",
    "    :param img_size: size of input images during training\n",
    "    :type img_size: tuple(int)\n",
    "    :return: ranking network model\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "    # Create feature extractor from VGG19\n",
    "    feature_extractor = VGG19(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    for layer in feature_extractor.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add dense layers on top of the feature extractor\n",
    "    inp = Input(shape=(img_size, img_size, 3), name='input_image')\n",
    "    base = feature_extractor(inp)\n",
    "    base = Flatten(name='Flatten')(base)\n",
    "\n",
    "    # Block 1\n",
    "    base = Dense(32, activation='relu', name='Dense_1')(base)\n",
    "    base = BatchNormalization(name='BN1')(base)\n",
    "    base = Dropout(0.490, name='Drop_1')(base)\n",
    "\n",
    "    # Block 2\n",
    "    base = Dense(128, activation='relu', name='Dense_2')(base)\n",
    "    base = BatchNormalization(name='BN2')(base)\n",
    "    base = Dropout(0.368, name='Drop_2')(base)\n",
    "\n",
    "    # Final dense\n",
    "    base = Dense(1, name=\"Dense_Output\")(base)\n",
    "    base_network = Model(inp, base, name='Scoring_model')\n",
    "    return base_network\n",
    "\n",
    "\n",
    "def create_meta_network(img_size, weights=None):\n",
    "    \"\"\"\n",
    "    Create meta network which is used to to teach the ranking network.\n",
    "\n",
    "    :param img_size: dimension of input images during training.\n",
    "    :type img_size: tuple(int)\n",
    "    :param weights: path to the weights use for initialization\n",
    "    :type weights: str\n",
    "    :return: meta network model\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the two input branches\n",
    "    input_left = Input(shape=(img_size, img_size, 3), name='left_input')\n",
    "    input_right = Input(shape=(img_size, img_size, 3), name='right_input')\n",
    "    base_network = create_ranking_network(img_size)\n",
    "    left_score = base_network(input_left)\n",
    "    right_score = base_network(input_right)\n",
    "\n",
    "    # Subtract scores\n",
    "    diff = Subtract()([left_score, right_score])\n",
    "\n",
    "    # Pass difference through sigmoid function.\n",
    "    prob = Activation(\"sigmoid\", name=\"Activation_sigmoid\")(diff)\n",
    "    model = Model(inputs=[input_left, input_right], outputs= prob, name=\"Meta_Model\")\n",
    "\n",
    "    if weights:\n",
    "        print('Loading weights ...')\n",
    "        model.load_weights(weights)\n",
    "\n",
    "\n",
    "    sgd = SGD(learning_rate=1e-6, decay=1e-6, momentum=0.393, nesterov=True)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.000001), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "meta_network = create_meta_network(224)\n",
    "meta_network.summary()\n",
    "meta_network.fit(X_train, y_train, batch_size=16, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bef3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_model = meta_network.get_layer('Scoring_model')\n",
    "ranking_model.save_weights('ranking_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f464dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_model = create_ranking_network(224)\n",
    "ranking_model.load_weights('ranking_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict scores for the images\n",
    "scores = ranking_model.predict(X_pred[0])\n",
    "\n",
    "# Create an array of indices to maintain the original order\n",
    "indices = np.arange(len(scores))\n",
    "\n",
    "# Sort the indices based on the scores in descending order\n",
    "sorted_indices = sorted(indices, key=lambda x: scores[x], reverse=True)\n",
    "\n",
    "# Set the number of columns for the grid\n",
    "num_columns = 5\n",
    "\n",
    "# Calculate the number of rows based on the number of images and columns\n",
    "num_images = len(X_pred[0])\n",
    "num_rows = int(np.ceil(num_images / num_columns))\n",
    "\n",
    "# Create a figure and axes for the grid\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 3*num_rows))\n",
    "\n",
    "# Iterate over the sorted indices and plot the images in the grid\n",
    "for i, index in enumerate(sorted_indices):\n",
    "    row = i // num_columns\n",
    "    col = i % num_columns\n",
    "\n",
    "    # Plot the image with the corresponding score\n",
    "    ax = axes[row, col]\n",
    "    ax.imshow(X_pred[0][index])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Score: {scores[index]}\")\n",
    "\n",
    "# Adjust the layout and display the grid of images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
